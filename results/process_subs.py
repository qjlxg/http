import requests
import base64
import re
import csv
import os
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

# è¾“å…¥è¾“å‡ºé…ç½®
INPUT_FILE = "results/subscriptions.txt"
OUTPUT_NODES = "results/nodes.txt"
OUTPUT_CSV = "results/statistics.csv"

def fetch_and_count(url):
    url = url.strip()
    if not url or url.startswith("#"):
        return None
    
    headers = {
        "User-Agent": "Clash/1.0" # æ¨¡æ‹Ÿ Clash å®¢æˆ·ç«¯ï¼Œæœ‰äº›æœºåœºå±è”½æ™®é€šçˆ¬è™«
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            content = response.text.strip()
            # æœºåœºè¿”å›é€šå¸¸æ˜¯ Base64
            try:
                decoded = base64.b64decode(content + '=' * (-len(content) % 4)).decode('utf-8')
                # ç»Ÿè®¡èŠ‚ç‚¹æ•°é‡ï¼ˆé€šå¸¸ä¸€è¡Œä¸€ä¸ªèŠ‚ç‚¹ï¼Œä»¥ vmess://, ss://, trojan:// å¼€å¤´ï¼‰
                nodes = [n for n in decoded.splitlines() if "://" in n]
                return {
                    "url": url,
                    "count": len(nodes),
                    "status": "Success",
                    "data": nodes
                }
            except:
                # æœ‰äº›è¿”å›çš„æ˜¯æ˜æ–‡ yaml/confï¼Œç›´æ¥ç»Ÿè®¡åŒ…å«èŠ‚ç‚¹çš„è¡Œ
                nodes = [n for n in content.splitlines() if "://" in n]
                return {
                    "url": url,
                    "count": len(nodes),
                    "status": "Partial/Plain",
                    "data": nodes
                }
    except Exception as e:
        return {"url": url, "count": 0, "status": f"Error: {str(e)[:20]}", "data": []}
    return {"url": url, "count": 0, "status": "Failed", "data": []}

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"File {INPUT_FILE} not found!")
        return

    with open(INPUT_FILE, "r") as f:
        urls = [line for line in f if line.startswith("http")]

    print(f"ğŸš€ å¼€å§‹å¤„ç† {len(urls)} æ¡é“¾æ¥...")
    
    all_nodes = []
    stats = []

    # ä½¿ç”¨ 20 ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=20) as executor:
        results = list(executor.map(fetch_and_count, urls))

    for res in results:
        if res:
            stats.append([res["url"], res["count"], res["status"]])
            all_nodes.extend(res["data"])

    # 1. ä¿å­˜æ‰€æœ‰èŠ‚ç‚¹ï¼ˆå»é‡ï¼‰
    unique_nodes = list(set(all_nodes))
    os.makedirs("results", exist_ok=True)
    with open(OUTPUT_NODES, "w", encoding="utf-8") as f:
        f.write("\n".join(unique_nodes))

    # 2. ç”Ÿæˆç»Ÿè®¡ CSV
    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["è®¢é˜…é“¾æ¥", "è·å–èŠ‚ç‚¹æ•°", "çŠ¶æ€"])
        writer.writerows(stats)

    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š æ€»è®¡è·å–ç‹¬ç«‹èŠ‚ç‚¹: {len(unique_nodes)} ä¸ª")
    print(f"ğŸ“ ç»Ÿè®¡æŠ¥è¡¨å·²ä¿å­˜è‡³: {OUTPUT_CSV}")

if __name__ == "__main__":
    main()
