import requests
import re
import os
import urllib.parse
import random
import time
from datetime import datetime

# 1. ç­›é€‰å‡ºçš„æœ€å¼º SearXNG å®ä¾‹ (æ¥è‡ªä½ æä¾›çš„å®æ—¶æ•°æ®)
SEARCH_INSTANCES = [
    "https://searxng.site/search",
    "https://searx.tiekoetter.com/search",
    "https://searx.rhscz.eu/search",
    "https://find.xenorio.xyz/search",
    "https://search.indst.eu/search",
    "https://searx.dresden.network/search",
    "https://paulgo.io/search",
    "https://searx.perennialte.ch/search"
]

# 2. æœç´¢å…³é”®è¯ (Dorks)
DORKS = [
    'inurl:"/api/v1/client/subscribe?token="',
    '"/api/v1/client/subscribe?token=" site:pastebin.com',
    '"/api/v1/client/subscribe?token=" site:t.me',
    '"/api/v1/client/subscribe?token=" site:github.com'
]

# 3. è®¢é˜…é“¾æ¥æ­£åˆ™
SUB_PATTERN = r'https?://[^\s^"\'\(\)]+/api/v1/client/subscribe\?token=[a-zA-Z0-9]{16,32}'

def fetch_from_instance(instance, query):
    encoded_query = urllib.parse.quote(query)
    # å¼ºåˆ¶è¯·æ±‚ Google å¼•æ“ç»“æœï¼Œå¾ˆå¤šå®ä¾‹é»˜è®¤ä¸å¼€å¯ Google
    url = f"{instance}?q={encoded_query}&engines=google,bing,duckduckgo&format=json"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    try:
        # ä½¿ç”¨ json æ ¼å¼è·å–ç»“æœé€šå¸¸æ¯”è§£æ HTML æ›´ç¨³å®š
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code == 200:
            # ç›´æ¥åœ¨è¿”å›çš„æ–‡æœ¬ä¸­æœç´¢æ­£åˆ™
            links = re.findall(SUB_PATTERN, response.text)
            return set(links)
    except:
        pass
    return set()

def main():
    all_found = set()
    print(f"[{datetime.now()}] ğŸš€ æ­£åœ¨åˆ©ç”¨å®æ—¶ä¼˜è´¨å®ä¾‹è¿›è¡Œæ”¶å‰²...")

    for dork in DORKS:
        # æ¯ä¸ª Dork éšæœºé€‰ 3 ä¸ªå®ä¾‹å°è¯•ï¼Œå¢åŠ æˆåŠŸç‡å¹¶é˜²æ­¢è¢«å°
        selected_instances = random.sample(SEARCH_INSTANCES, 3)
        for ins in selected_instances:
            print(f"ğŸ” æ­£åœ¨ä½¿ç”¨ [{ins}] æœç´¢: {dork}")
            links = fetch_from_instance(ins, dork)
            if links:
                print(f"   âœ¨ å‘ç° {len(links)} æ¡é“¾æ¥!")
                all_found.update(links)
            time.sleep(1) # ç¨å¾®åœé¡¿

    # ä¿å­˜ç»“æœ
    os.makedirs("results", exist_ok=True)
    file_path = "results/subscriptions.txt"
    
    # è¿‡æ»¤æ‰é‡å¤å’Œå·²çŸ¥çš„æµ‹è¯•é“¾æ¥
    final_list = sorted([l for l in all_found if "example.com" not in l])

    with open(file_path, "w", encoding="utf-8") as f:
        f.write(f"# é‡‡é›†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (Shanghai)\n")
        f.write(f"# èµ„æºæ¥æº: SearXNG Cluster (High Uptime Instances)\n")
        f.write(f"# æœ‰æ•ˆé“¾æ¥æ€»æ•°: {len(final_list)}\n\n")
        for link in final_list:
            f.write(link + "\n")

    print(f"\nâœ… ä»»åŠ¡å®Œæˆï¼å…±æ•è· {len(final_list)} æ¡æœ‰æ•ˆè®¢é˜…ã€‚")

if __name__ == "__main__":
    main()
