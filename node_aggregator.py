import requests
import re
import os
import time
import socket
import json
import base64
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

# --- é…ç½®åŒº ---
GITHUB_TOKEN = os.getenv("MY_GITHUB_TOKEN")
OUTPUT_DIR = "results"
# å®Œå–„åè®®åŒ¹é…æ­£åˆ™ï¼Œç¡®ä¿èƒ½æŠ“å–åˆ°å¸¦å‚æ•°çš„å¤æ‚é“¾æ¥
NODE_PATTERN = r'(vmess|vless|ss|ssr|trojan|tuic|hysteria2|hysteria)://[^\s^"\'\(\)]+'
# æ‰©å……é»‘åå•ï¼Œè¿‡æ»¤æ‰æ›´å¤šåƒåœ¾èŠ‚ç‚¹
BAD_KEYWORDS = ['è¿‡æœŸ', 'æµé‡', 'è€—å°½', 'åˆ°æœŸ', '0GB', 'å‰©ä½™', 'å®˜ç½‘', 'ç»´æŠ¤', 'é‡ç½®', 'æµ‹è¯•', 'è´­ä¹°']

# ç²¾å“èŠ‚ç‚¹æ± ï¼ˆç›´æ¥å­˜æ”¾èŠ‚ç‚¹çš„æ–‡ä»¶åœ°å€ï¼‰
RAW_NODE_SOURCES = [
    "https://raw.githubusercontent.com/vless-free/free/main/v2ray",
    "https://raw.githubusercontent.com/freefq/free/master/v2ray",
    "https://raw.githubusercontent.com/Pawdroid/Free-v2ray/main/v2ray.txt",
    "https://raw.githubusercontent.com/LonUp/NodeList/main/latest/all_export.txt",
    "https://raw.githubusercontent.com/mueiba/free-nodes/main/nodes.txt"
]

# GitHub æœç´¢ Dorksï¼šé”å®šåŒ…å«åŸå§‹èŠ‚ç‚¹çš„æ–‡æœ¬æ–‡ä»¶
GITHUB_DORKS = [
    'extension:txt "vmess://"',
    'extension:txt "vless://"',
    'extension:txt "trojan://"',
    'extension:txt "hysteria2://"',
    'filename:nodes.txt "ss://"',
    'filename:sub.txt "vmess://"',
    'filename:README.md "æ›´æ–°æ—¶é—´" "vmess://"'
]

# --- æ ¸å¿ƒè¿‡æ»¤é€»è¾‘ ---

def check_tcp_alive(node_url):
    """TCP æ¢æµ‹ï¼šç¡®ä¿èŠ‚ç‚¹æœåŠ¡å™¨æ˜¯é€šçš„"""
    try:
        host, port = None, None
        if node_url.startswith(('ss://', 'trojan://', 'vless://', 'ssr://', 'hysteria2://', 'hysteria://', 'tuic://')):
            # å…¼å®¹æ ‡å‡†åè®®æ ¼å¼
            if '@' in node_url:
                part = node_url.split('@')[1].split('#')[0].split('?')[0]
                if ':' in part:
                    host, port = part.split(':')[0], int(part.split(':')[1])
        elif node_url.startswith('vmess://'):
            # è§£ç  vmess json æ ¼å¼
            b64_data = node_url.replace('vmess://', '')
            b64_data += '=' * (-len(b64_data) % 4)
            data = json.loads(base64.b64decode(b64_data).decode('utf-8'))
            host, port = data['add'], int(data['port'])
        
        if host and port:
            # å»ºç«‹ç‰©ç†è¿æ¥æµ‹è¯•ï¼Œè¶…æ—¶è®¾ä¸º 1.5s ä»¥è¿‡æ»¤æ‰é«˜å»¶è¿Ÿåƒåœ¾
            with socket.create_connection((host, port), timeout=1.5):
                return True
    except:
        pass
    return False

def get_github_raw_nodes():
    """åˆ©ç”¨ API æœç´¢åŒ…å«åŸå§‹èŠ‚ç‚¹çš„æ–‡ä»¶å†…å®¹"""
    if not GITHUB_TOKEN: 
        print("âš ï¸ è­¦å‘Š: æœªå‘ç° MY_GITHUB_TOKENï¼Œå°†è·³è¿‡ GitHub API æœç´¢ã€‚")
        return set()
    
    found_nodes = set()
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3.text-match+json"
    }
    
    for dork in GITHUB_DORKS:
        try:
            print(f"ğŸ” æ­£åœ¨æ‰§è¡Œ Dork: {dork}")
            url = f"https://api.github.com/search/code?q={dork}&sort=indexed&order=desc"
            response = requests.get(url, headers=headers, timeout=20)
            
            if response.status_code == 200:
                items = response.json().get('items', [])
                for item in items:
                    raw_url = item['html_url'].replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
                    try:
                        content = requests.get(raw_url, timeout=10).text
                        nodes = re.findall(NODE_PATTERN, content)
                        found_nodes.update(nodes)
                    except: continue
            elif response.status_code == 403:
                print("ğŸš« API é€Ÿç‡å—é™ï¼Œç¨åç»§ç»­...")
                time.sleep(10)
            
            time.sleep(3) # éµå®ˆ API è°ƒç”¨é¢‘ç‡
        except Exception as e:
            print(f"âš ï¸ æœç´¢ä»»åŠ¡ä¸­æ–­: {e}")
    return found_nodes

def main():
    start_time = datetime.now()
    print(f"[{start_time}] ğŸ›°ï¸ å¯åŠ¨å…¨ç½‘åŸå§‹èŠ‚ç‚¹æ”¶å‰²æ¨¡å¼...")
    
    all_collected = set()

    # 1. æŠ“å–å·²çŸ¥ç²¾å“èŠ‚ç‚¹æ± 
    for src in RAW_NODE_SOURCES:
        print(f"ğŸ“¡ æ‰«æç²¾å“æº: {src}")
        try:
            content = requests.get(src, timeout=15).text
            nodes = re.findall(NODE_PATTERN, content)
            all_collected.update(nodes)
            print(f"   âœ¨ å‘ç° {len(nodes)} ä¸ªèŠ‚ç‚¹å€™é€‰")
        except: pass

    # 2. æœç´¢ GitHub ä¸Šçš„éšè—èŠ‚ç‚¹æ–‡ä»¶
    all_collected.update(get_github_raw_nodes())

    # 3. é“è¡€æ¸…æ´—ä¸ TCP éªŒè¯
    print(f"âš™ï¸ åŸå§‹è·å– {len(all_collected)} æ¡æ•°æ®ï¼Œå¼€å§‹æ´»ä½“æ£€æµ‹...")
    
    def verify_node(node):
        # æ’é™¤é»‘åå•å…³é”®è¯
        if any(word in node for word in BAD_KEYWORDS): return None
        # æ¢æµ‹å­˜æ´»ï¼Œä¸é€šçš„ç›´æ¥æ‰”æ‰
        if check_tcp_alive(node): return node
        return None

    # å¹¶å‘ 50 çº¿ç¨‹æµ‹é€Ÿ
    with ThreadPoolExecutor(max_workers=50) as executor:
        results = list(executor.map(verify_node, list(all_collected)))
        final_nodes = [r for r in results if r]

    # 4. ä¿å­˜ç»“æœ
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(f"{OUTPUT_DIR}/nodes.txt", "w", encoding="utf-8") as f:
        # å»é‡å¹¶æŒ‰ç…§å­—å…¸åºæ’åˆ—
        unique_nodes = sorted(list(set(final_nodes)))
        f.write("\n".join(unique_nodes))

    print(f"âœ… å®Œæˆï¼æœ€ç»ˆæ•è·çœŸÂ·æ´»èŠ‚ç‚¹: {len(unique_nodes)} ä¸ª")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³ {OUTPUT_DIR}/nodes.txt")
    print(f"â±ï¸ æ€»è€—æ—¶: {datetime.now() - start_time}")

if __name__ == "__main__":
    main()
