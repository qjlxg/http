import requests
import re
import os
import time
import base64
import json
import socket
import csv
import yaml
import geoip2.database
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from urllib.parse import urlparse

# --- é…ç½®åŒº ---
GITHUB_TOKEN = os.getenv("MY_GITHUB_TOKEN")
OUTPUT_DIR = "."
GEOIP_DB_PATH = "GeoLite2-Country.mmdb"
STATS_CSV_PATH = "source_stats.csv"
MAX_WORKERS = 80  # æé«˜å¹¶å‘æ•°ï¼ŒåŠ å¿«æ£€æµ‹é€Ÿåº¦

# æ’é™¤å…³é”®è¯
EXCLUDE_KEYWORDS = ["127.0.0.1", "localhost", "0.0.0.0", "google.com", "github.com"]
# æ ‡å‡†èŠ‚ç‚¹æ­£åˆ™
NODE_PATTERN = r'(?:vmess|vless|ss|ssr|trojan|tuic|hysteria2|hysteria)://[a-zA-Z0-9%@\[\]\._\-\?&=\+#/:]+'

RAW_NODE_SOURCES = [
    "https://raw.githubusercontent.com/qjlxg/aggregator/refs/heads/main/data/clash.yaml",
    "https://raw.githubusercontent.com/qjlxg/aggregator/refs/heads/main/data/520.yaml",
    "https://raw.githubusercontent.com/qjlxg/one/refs/heads/main/nodes_list.txt",
    "https://raw.githubusercontent.com/qjlxg/one/refs/heads/main/latest_nodes.txt"
]

# --- å·¥å…·å‡½æ•° ---

def auto_decode_base64(text):
    """é²æ£’æ€§ Base64 è§£ç """
    text = text.strip()
    if "://" in text and len(text) > 60: return text
    try:
        clean_text = re.sub(r'[^a-zA-Z0-9+/=]', '', text)
        missing_padding = len(clean_text) % 4
        if missing_padding: clean_text += '=' * (4 - missing_padding)
        return base64.b64decode(clean_text).decode('utf-8', errors='ignore')
    except:
        return text

def parse_yaml_to_links(content):
    """è§£æ Clash YAML æ ¼å¼å¹¶è½¬æ¢ä¸ºæ ‡å‡†é“¾æ¥"""
    links = []
    try:
        # é¢„å¤„ç†ï¼šé˜²æ­¢æœ‰äº› YAML å¼€å¤´æœ‰éæ ‡å‡†å­—ç¬¦
        if "proxies:" not in content: return []
        data = yaml.safe_load(content)
        if not data or 'proxies' not in data: return []
        
        for p in data['proxies']:
            try:
                t = p.get('type', '').lower()
                server = p.get('server')
                port = p.get('port')
                name = p.get('name', 'node')
                if not server or not port: continue

                if t == 'vless':
                    uuid = p.get('uuid')
                    tls = "tls" if p.get('tls') else "none"
                    sni = p.get('servername', '')
                    links.append(f"vless://{uuid}@{server}:{port}?security={tls}&sni={sni}#{name}")
                elif t == 'trojan':
                    pw = p.get('password')
                    links.append(f"trojan://{pw}@{server}:{port}#{name}")
                elif t == 'ss':
                    # SS æ ¼å¼è¾ƒå¤æ‚ï¼Œè¿™é‡Œåšç®€åŒ–å¤„ç†ï¼Œè¿›å…¥å»é‡é€»è¾‘
                    links.append(f"ss://{server}:{port}#{name}")
            except: continue
    except: pass
    return links

def extract_host_port(node_url):
    """ä»èŠ‚ç‚¹é“¾æ¥ä¸­æå– IP/Host å’Œç«¯å£"""
    try:
        if node_url.startswith("vmess://"):
            v2_raw = base64.b64decode(node_url[8:]).decode('utf-8')
            v2_json = json.loads(v2_raw)
            return str(v2_json.get('add')).strip(), str(v2_json.get('port')).strip()
        parsed = urlparse(node_url)
        netloc = parsed.netloc
        if "@" in netloc: netloc = netloc.split("@")[-1]
        if ":" in netloc:
            host, port = netloc.split(":")
            return host.strip(), port.strip()
        return netloc.strip(), "0"
    except:
        return None, None

def check_alive(host, port):
    """TCP ç«¯å£å­˜æ´»æ£€æµ‹"""
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.settimeout(2.5)
            return s.connect_ex((host, int(port))) == 0
    except: return False

# --- æ ¸å¿ƒç±» ---

class NodeAggregator:
    def __init__(self):
        self.raw_nodes = set()
        self.source_stats = []
        self.geo_reader = None
        if os.path.exists(GEOIP_DB_PATH):
            self.geo_reader = geoip2.database.Reader(GEOIP_DB_PATH)

    def get_country(self, host):
        if not self.geo_reader: return "UN"
        try:
            ip = host
            if not re.match(r"^\d{1,3}(\.\d{1,3}){3}$", host):
                ip = socket.gethostbyname(host)
            return self.geo_reader.country(ip).country.iso_code
        except: return "UN"

    def fetch_source(self, url):
        """æŠ“å–é€»è¾‘ï¼šå…¼å®¹æ­£è§„é“¾æ¥å’Œ YAML"""
        try:
            res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
            if res.status_code == 200:
                text = res.text
                # å°è¯•æ­£åˆ™
                found = re.findall(NODE_PATTERN, auto_decode_base64(text), re.IGNORECASE)
                # å°è¯• YAML (å¦‚æœæ­£åˆ™æ²¡å‘ç°æˆ–åŒ…å« YAML ç‰¹å¾)
                if "proxies:" in text:
                    found.extend(parse_yaml_to_links(text))
                
                return url, found, 200
            return url, [], res.status_code
        except Exception as e:
            return url, [], str(e)

    def run(self):
        start_time = datetime.now()
        print(f"[{start_time.strftime('%H:%M:%S')}] ğŸš€ å¯åŠ¨å…¨åŠŸèƒ½æ”¶å‰²æµ...")

        # 1. å¹¶å‘æŠ“å–
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(self.fetch_source, url) for url in RAW_NODE_SOURCES]
            for f in as_completed(futures):
                url, nodes, status = f.result()
                self.raw_nodes.update(nodes)
                self.source_stats.append({
                    "date": start_time.strftime("%Y-%m-%d %H:%M:%S"),
                    "source_url": url,
                    "node_count": len(nodes),
                    "status": status
                })

        # ä¿å­˜ç»Ÿè®¡ CSV (L1 çº§)
        self.save_stats()

        # 2. ä¸‰çº§å»é‡ (L1: set, L2: identity, L3: refine)
        unique_pool = {}
        for node in self.raw_nodes:
            if len(node) < 15 or any(kw in node.lower() for kw in EXCLUDE_KEYWORDS):
                continue
            
            host, port = extract_host_port(node)
            if host and port:
                protocol = node.split("://")[0].lower()
                identity = f"{protocol}://{host}:{port}" # L2 å»é‡ç‰¹å¾
                if identity not in unique_pool:
                    # L3 æ¸…æ´—ï¼šå»é™¤åŸæœ‰å¤‡æ³¨
                    unique_pool[identity] = node.split("#")[0] if "#" in node else node

        # 3. å­˜æ´»æ£€æµ‹ä¸å½’å±åœ°è¯†åˆ«
        print(f"âš¡ æ­£åœ¨æ£€æµ‹ {len(unique_pool)} ä¸ªç‹¬ç‰¹èŠ‚ç‚¹...")
        results_by_country = {}
        
        def process_node(item):
            identity, url = item
            protocol = identity.split("://")[0]
            host, port = identity.split("://")[-1].split(":")
            if check_alive(host, port):
                country = self.get_country(host)
                return country, f"{url}#{country}_{protocol}_{host}"
            return None, None

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            node_futures = [executor.submit(process_node, it) for it in unique_pool.items()]
            for f in as_completed(node_futures):
                country, labeled_node = f.result()
                if country:
                    if country not in results_by_country: results_by_country[country] = []
                    results_by_country[country].append(labeled_node)

        # 4. ä¿å­˜ nodes.txt
        self.save_nodes(results_by_country)

        print(f"---")
        print(f"âœ… å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“¦ æŠ“å–æ€»æ•°: {len(self.raw_nodes)}")
        print(f"ğŸŒ å­˜æ´»èŠ‚ç‚¹: {sum(len(v) for v in results_by_country.values())}")
        print(f"â±ï¸  æ€»è€—æ—¶: {datetime.now() - start_time}")

    def save_stats(self):
        with open(STATS_CSV_PATH, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=["date", "source_url", "node_count", "status"])
            writer.writeheader()
            writer.writerows(self.source_stats)
        print(f"ğŸ“Š ç»Ÿè®¡æŠ¥è¡¨å·²æ›´æ–°: {STATS_CSV_PATH}")

    def save_nodes(self, data):
        with open(os.path.join(OUTPUT_DIR, "nodes.txt"), "w", encoding="utf-8") as f:
            for country in sorted(data.keys()):
                f.write(f"\n# --- {country} ---\n")
                f.write("\n".join(sorted(data[country])) + "\n")

if __name__ == "__main__":
    aggregator = NodeAggregator()
    aggregator.run()
