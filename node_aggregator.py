import requests
import re
import os
import time
import base64
import json
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

# --- é…ç½®åŒº ---
GITHUB_TOKEN = os.getenv("MY_GITHUB_TOKEN")
OUTPUT_DIR = "results"
# åè®®åŒ¹é…æ­£åˆ™
NODE_PATTERN = r'(vmess|vless|ss|ssr|trojan|tuic|hysteria2|hysteria)://[^\s^"\'\(\)]+'

# èŠ‚ç‚¹æ± ï¼ˆç›´æ¥å­˜æ”¾èŠ‚ç‚¹çš„æ–‡ä»¶åœ°å€ï¼‰
RAW_NODE_SOURCES = [
    "https://raw.githubusercontent.com/vless-free/free/main/v2ray",
    "https://raw.githubusercontent.com/freefq/free/master/v2ray",
    "https://raw.githubusercontent.com/Pawdroid/Free-v2ray/main/v2ray.txt",
    "https://raw.githubusercontent.com/LonUp/NodeList/main/latest/all_export.txt",
    "https://raw.githubusercontent.com/mueiba/free-nodes/main/nodes.txt",
    "https://raw.githubusercontent.com/v2ray-free/free/main/v2ray",
    "https://raw.githubusercontent.com/StaySleepless/free-nodes/main/nodes.txt",
    "https://raw.githubusercontent.com/tbbatbb/Proxy/master/dist/v2ray.txt"
]

GITHUB_DORKS = [
    'extension:txt "vmess://"',
    'extension:txt "vless://"',
    'extension:txt "trojan://"',
    'filename:nodes.txt "ss://"',
    'filename:README.md "æ›´æ–°æ—¶é—´" "vmess://"'
]

def auto_decode_base64(text):
    """å°è¯•å„ç§å§¿åŠ¿è§£ç å†…å®¹"""
    text = text.strip()
    # 1. å·²ç»æ˜¯æ˜æ–‡èŠ‚ç‚¹åˆ—è¡¨
    if "://" in text:
        return text
    # 2. å°è¯• Base64 è§£ç 
    try:
        # è¡¥é½å¡«å……
        missing_padding = len(text) % 4
        if missing_padding:
            text += '=' * (4 - missing_padding)
        decoded = base64.b64decode(text).decode('utf-8')
        return decoded
    except:
        return text

def get_github_raw_nodes():
    if not GITHUB_TOKEN: return set()
    found_nodes = set()
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    for dork in GITHUB_DORKS:
        try:
            url = f"https://api.github.com/search/code?q={dork}&sort=indexed"
            res = requests.get(url, headers=headers, timeout=20).json()
            items = res.get('items', [])
            print(f"ğŸ” Dork [{dork}] å‘½ä¸­ {len(items)} ä¸ªæ–‡ä»¶")
            for item in items:
                raw_url = item['html_url'].replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
                try:
                    c = requests.get(raw_url, timeout=5).text
                    decoded_c = auto_decode_base64(c)
                    nodes = re.findall(NODE_PATTERN, decoded_c)
                    found_nodes.update(nodes)
                except: continue
            time.sleep(2) # é¿å… API é™åˆ¶
        except: pass
    return found_nodes

def fetch_source(src):
    """ä¸‹è½½å¹¶è§£æå•ä¸ªæº"""
    try:
        print(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚: {src}")
        res = requests.get(src, timeout=10)
        if res.status_code == 200:
            content = auto_decode_base64(res.text)
            nodes = re.findall(NODE_PATTERN, content)
            print(f"   âœ¨ ä» {src[-15:]} æå–åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
            return nodes
    except:
        return []

def main():
    start_time = datetime.now()
    print(f"[{start_time}] ğŸš€ å¯åŠ¨å…¨é‡æ”¶å‰²æ¨¡å¼ï¼ˆè·³è¿‡ TCP éªŒè¯ï¼‰...")
    
    all_raw = set()

    # 1. å¹¶å‘æŠ“å–å¤–éƒ¨æº
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_source, RAW_NODE_SOURCES))
        for nodes in results:
            if nodes: all_raw.update(nodes)

    # 2. æŠ“å– GitHub æœç´¢ç»“æœ
    print("ğŸ” å¯åŠ¨ GitHub æ·±åº¦æŒ–æ˜...")
    all_raw.update(get_github_raw_nodes())

    # 3. ç»“æœå»é‡å¹¶ä¿å­˜ï¼ˆä¸å†è¿›è¡Œ check_tcp_aliveï¼‰
    unique_nodes = sorted(list(set(all_raw)))
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(f"{OUTPUT_DIR}/nodes.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(unique_nodes))

    print(f"âœ… å®Œæˆï¼å…±æ”¶è·èŠ‚ç‚¹: {len(unique_nodes)} ä¸ª")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³ {OUTPUT_DIR}/nodes.txt")
    print(f"â±ï¸ è€—æ—¶: {datetime.now() - start_time}")

if __name__ == "__main__":
    main()
