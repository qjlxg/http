import requests
import re
import os
import time
import base64
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

# --- é…ç½®åŒº ---
GITHUB_TOKEN = os.getenv("MY_GITHUB_TOKEN")
OUTPUT_DIR = "results"

# --- æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨ (?:...) éæ•è·åˆ†ç»„ï¼Œç¡®ä¿åŒ¹é…å®Œæ•´é“¾æ¥è€Œéä»…åè®®å ---
NODE_PATTERN = r'(?:vmess|vless|ss|ssr|trojan|tuic|hysteria2|hysteria)://[^\s^"\'\(\)]+'

# ç²¾å“èŠ‚ç‚¹æ± 
RAW_NODE_SOURCES = [
    "https://raw.githubusercontent.com/vless-free/free/main/v2ray",
    "https://raw.githubusercontent.com/freefq/free/master/v2ray",
    "https://raw.githubusercontent.com/Pawdroid/Free-v2ray/main/v2ray.txt",
    "https://raw.githubusercontent.com/LonUp/NodeList/main/latest/all_export.txt",
    "https://raw.githubusercontent.com/mueiba/free-nodes/main/nodes.txt",
    "https://raw.githubusercontent.com/StaySleepless/free-nodes/main/nodes.txt",
    "https://raw.githubusercontent.com/tbbatbb/Proxy/master/dist/v2ray.txt",
    "https://raw.githubusercontent.com/v2ray-free/free/main/v2ray"
]

GITHUB_DORKS = [
    'extension:txt "vmess://"',
    'extension:txt "vless://"',
    'extension:txt "ssr://"',
    'extension:txt "hysteria2://"',
    'filename:nodes.txt "ss://"',
    'filename:README.md "æ›´æ–°æ—¶é—´" "vmess://"'
]

def auto_decode_base64(text):
    """è‡ªåŠ¨æ¢æµ‹å¹¶è§£ç å†…å®¹"""
    text = text.strip()
    if "://" in text:
        return text
    try:
        missing_padding = len(text) % 4
        if missing_padding:
            text += '=' * (4 - missing_padding)
        # è§£ç å¹¶å¿½ç•¥æ— æ³•è¯†åˆ«çš„å­—ç¬¦
        return base64.b64decode(text).decode('utf-8', errors='ignore')
    except:
        return text

def fetch_from_github():
    if not GITHUB_TOKEN:
        return set()
    
    found = set()
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    for dork in GITHUB_DORKS:
        try:
            print(f"ğŸ” æ­£åœ¨æ£€ç´¢ GitHub: {dork}")
            url = f"https://api.github.com/search/code?q={dork}&sort=indexed"
            res = requests.get(url, headers=headers, timeout=15).json()
            for item in res.get('items', []):
                raw_url = item['html_url'].replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
                try:
                    content = requests.get(raw_url, timeout=5).text
                    # å…³é”®ä¿®å¤ç‚¹ï¼šå…ˆè§£ç å†åŒ¹é…
                    decoded_content = auto_decode_base64(content)
                    matches = re.findall(NODE_PATTERN, decoded_content, re.IGNORECASE)
                    found.update(matches)
                except: continue
            time.sleep(2)
        except: pass
    return found

def fetch_from_sources(url):
    try:
        print(f"ğŸ“¡ æŠ“å–æº: {url}")
        res = requests.get(url, timeout=10)
        if res.status_code == 200:
            content = auto_decode_base64(res.text)
            matches = re.findall(NODE_PATTERN, content, re.IGNORECASE)
            print(f"   ğŸ“Š æå–åˆ° {len(matches)} ä¸ªå®Œæ•´é“¾æ¥")
            return matches
    except:
        pass
    return []

def main():
    start_time = datetime.now()
    print(f"[{start_time}] ğŸš€ å¯åŠ¨å…¨é‡èŠ‚ç‚¹æ”¶å‰²ï¼ˆä¿®æ­£æ­£åˆ™åˆ†ç»„é—®é¢˜ï¼‰...")
    
    all_nodes = set()

    # 1. æŠ“å–å¤–éƒ¨èšåˆæº
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_from_sources, RAW_NODE_SOURCES))
        for nodes in results:
            if nodes: all_nodes.update(nodes)

    # 2. æŠ“å– GitHub æœç´¢ç»“æœ
    all_nodes.update(fetch_from_github())

    # 3. ç»“æœä¿å­˜
    unique_list = sorted(list(set(all_nodes)))
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(f"{OUTPUT_DIR}/nodes.txt", "w", encoding="utf-8") as f:
        # ç¡®ä¿æ¯è¡Œä¸€ä¸ªå®Œæ•´èŠ‚ç‚¹é“¾æ¥
        f.write("\n".join(unique_list))

    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“¦ æˆåŠŸæ”¶å‰²å®Œæ•´èŠ‚ç‚¹é“¾æ¥: {len(unique_list)} ä¸ª")
    if len(unique_list) > 0:
        print(f"ğŸ“ é¢„è§ˆç¬¬ä¸€ä¸ªèŠ‚ç‚¹: {unique_list[0][:50]}...")

if __name__ == "__main__":
    main()
