import requests
import re
import os
import time
import base64
import json
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from urllib.parse import urlparse

# --- é…ç½®åŒº ---
GITHUB_TOKEN = os.getenv("MY_GITHUB_TOKEN")
OUTPUT_DIR = "."

# æ’é™¤å…³é”®è¯ï¼šåŒ…å«è¿™äº›å†…å®¹çš„èŠ‚ç‚¹å°†è¢«ä¸¢å¼ƒï¼ˆå¦‚å®˜ç½‘åœ°å€ã€æœ¬åœ°å›ç¯åœ°å€ï¼‰
EXCLUDE_KEYWORDS = ["127.0.0.1", "localhost", "0.0.0.0", "google.com", "github.com"]

# èŠ‚ç‚¹åŒ¹é…æ­£åˆ™
NODE_PATTERN = r'(?:vmess|vless|ss|ssr|trojan|tuic|hysteria2|hysteria)://[a-zA-Z0-9%@\[\]\._\-\?&=\+#/:]+'

# ç²¾å“èŠ‚ç‚¹æ± 
RAW_NODE_SOURCES = [
    "https://raw.githubusercontent.com/vless-free/free/main/v2ray",
    "https://raw.githubusercontent.com/freefq/free/master/v2ray",
    "https://raw.githubusercontent.com/Pawdroid/Free-v2ray/main/v2ray.txt",
    "https://raw.githubusercontent.com/LonUp/NodeList/main/latest/all_export.txt",
    "https://raw.githubusercontent.com/mueiba/free-nodes/main/nodes.txt",
    "https://raw.githubusercontent.com/StaySleepless/free-nodes/main/nodes.txt",
    "https://raw.githubusercontent.com/tbbatbb/Proxy/master/dist/v2ray.txt",
    "https://raw.githubusercontent.com/v2ray-free/free/main/v2ray"
]

GITHUB_DORKS = [
    'extension:txt "vmess://"',
    'extension:txt "vless://"',
    'extension:txt "ssr://"',
    'extension:txt "hysteria2://"',
    'filename:nodes.txt "ss://"',
    'filename:README.md "æ›´æ–°æ—¶é—´" "vmess://"'
]

def extract_host_port(node_url):
    """æå–èŠ‚ç‚¹ä¸­çš„ (host, port) ç”¨äºå»é‡"""
    try:
        # å¤„ç† vmess (é€šå¸¸æ˜¯ Base64 åçš„ JSON)
        if node_url.startswith("vmess://"):
            v2_raw = base64.b64decode(node_url[8:]).decode('utf-8')
            v2_json = json.loads(v2_raw)
            return str(v2_json.get('add')).strip(), str(v2_json.get('port')).strip()
        
        # å¤„ç†å…¶ä»–åè®® (vless, ss, trojan, etc.)
        parsed = urlparse(node_url)
        netloc = parsed.netloc
        if "@" in netloc:
            netloc = netloc.split("@")[-1]
        
        if ":" in netloc:
            host, port = netloc.split(":")
            return host.strip(), port.strip()
        return netloc.strip(), "0"
    except:
        return None, None

def auto_decode_base64(text):
    """é²æ£’æ€§å¼ºçš„ Base64 è§£ç """
    text = text.strip()
    if "://" in text and len(text) > 60:
        return text
    try:
        clean_text = re.sub(r'[^a-zA-Z0-9+/=]', '', text)
        missing_padding = len(clean_text) % 4
        if missing_padding:
            clean_text += '=' * (4 - missing_padding)
        return base64.b64decode(clean_text).decode('utf-8', errors='ignore')
    except:
        return text

def fetch_from_sources(url):
    """ä»å•ä¸€ URL æŠ“å–å¹¶è§£æèŠ‚ç‚¹"""
    try:
        res = requests.get(url, timeout=15)
        if res.status_code == 200:
            content = auto_decode_base64(res.text)
            return re.findall(NODE_PATTERN, content, re.IGNORECASE)
    except:
        pass
    return []

def fetch_from_github():
    """é€šè¿‡ GitHub API æœç´¢æœ€æ–°èŠ‚ç‚¹"""
    if not GITHUB_TOKEN: return set()
    found = set()
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    for dork in GITHUB_DORKS:
        try:
            url = f"https://api.github.com/search/code?q={dork}&sort=indexed"
            res = requests.get(url, headers=headers, timeout=15).json()
            for item in res.get('items', []):
                raw_url = item['html_url'].replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
                try:
                    content = requests.get(raw_url, timeout=5).text
                    decoded_content = auto_decode_base64(content)
                    matches = re.findall(NODE_PATTERN, decoded_content, re.IGNORECASE)
                    found.update(matches)
                except: continue
            time.sleep(1) # é¿å…è§¦å‘ Rate Limit
        except: pass
    return found

def main():
    start_time = datetime.now()
    print(f"[{start_time}] ğŸš€ å¯åŠ¨å…¨é‡èŠ‚ç‚¹æ”¶å‰² (æ·±åº¦å»é‡ç‰ˆ)...")
    
    raw_nodes = set()

    # 1. å¹¶å‘æŠ“å–è®¢é˜…æº
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_from_sources, RAW_NODE_SOURCES))
        for nodes in results:
            if nodes: raw_nodes.update(nodes)

    # 2. æŠ“å– GitHub å‘ç°æº
    raw_nodes.update(fetch_from_github())

    # 3. æ·±åº¦è¿‡æ»¤ä¸åŸºäº (IP, Port) çš„å»é‡
    unique_pool = {} # Key: "host:port", Value: node_url
    
    for node in raw_nodes:
        # åŸºæœ¬é•¿åº¦è¿‡æ»¤
        if len(node) < 15:
            continue
            
        # å…³é”®è¯é»‘åå•è¿‡æ»¤ (127.0.0.1 ç­‰)
        if any(kw in node.lower() for kw in EXCLUDE_KEYWORDS):
            continue
            
        host, port = extract_host_port(node)
        if host and port:
            # åªæœ‰å½“è¯¥ IP:Port ç»„åˆç¬¬ä¸€æ¬¡å‡ºç°æ—¶æ‰åŠ å…¥
            identity = f"{host}:{port}"
            if identity not in unique_pool:
                unique_pool[identity] = node

    final_list = sorted(unique_pool.values())
    
    # 4. ä¿å­˜ç»“æœ
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    file_path = os.path.join(OUTPUT_DIR, "nodes.txt")
    
    with open(file_path, "w", encoding="utf-8") as f:
        f.write("\n".join(final_list))

    print(f"---")
    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“¦ åŸå§‹æŠ“å–æ€»æ•°: {len(raw_nodes)}")
    print(f"ğŸ›¡ï¸  (IP:Port) å»é‡åæœ‰æ•ˆæ€»æ•°: {len(final_list)}")
    print(f"â±ï¸  æ€»è€—æ—¶: {datetime.now() - start_time}")

if __name__ == "__main__":
    main()
