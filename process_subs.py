import requests
import base64
import re
import csv
import os
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

# æ–‡ä»¶è·¯å¾„
INPUT_FILE = "results/subscriptions.txt"
OUTPUT_NODES = "results/nodes.txt"
OUTPUT_CSV = "results/statistics.csv"

# å®šä¹‰åˆæ³•çš„åè®®å¤´
VALID_PROTOCOLS = ('vmess://', 'vless://', 'ss://', 'ssr://', 'trojan://', 'tuic://', 'hysteria2://', 'hysteria://')

def clean_and_validate_node(line):
    """æ¸…æ´—å•è¡Œæ•°æ®ï¼Œåªä¿ç•™åˆæ³•çš„èŠ‚ç‚¹å­—ç¬¦ä¸²"""
    line = line.strip()
    # å¿…é¡»ä»¥åˆæ³•åè®®å¼€å¤´ï¼Œä¸”ä¸èƒ½åŒ…å« HTML æ ‡ç­¾
    if line.startswith(VALID_PROTOCOLS) and '<' not in line and '{' not in line:
        return line
    return None

def fetch_content(url):
    url = url.strip()
    if not url or url.startswith("#"):
        return None
    
    headers = {"User-Agent": "Clash/1.0; v2rayN/6.23"}
    try:
        response = requests.get(url, headers=headers, timeout=12)
        if response.status_code == 200:
            raw_data = response.text.strip()
            
            # å°è¯• Base64 è§£ç 
            content = ""
            try:
                # è‡ªåŠ¨è¡¥å…¨å¹¶å°è¯•è§£ç 
                missing_padding = len(raw_data) % 4
                if missing_padding: raw_data += '=' * (4 - missing_padding)
                content = base64.b64decode(raw_data).decode('utf-8')
            except:
                # è§£ç å¤±è´¥åˆ™è§†ä¸ºæ˜æ–‡
                content = raw_data
            
            # æå–å¹¶æ¸…æ´—èŠ‚ç‚¹
            extracted_nodes = []
            for line in content.splitlines():
                node = clean_and_validate_node(line)
                if node:
                    extracted_nodes.append(node)
            
            return {"url": url, "count": len(extracted_nodes), "nodes": extracted_nodes}
    except:
        pass
    return {"url": url, "count": 0, "nodes": []}

def main():
    if not os.path.exists(INPUT_FILE):
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.startswith("http")]

    print(f"ğŸš€ æ­£åœ¨æ¸…æ´—å¹¶æå– {len(urls)} ä¸ªæº...")
    
    all_nodes = []
    stats = []

    with ThreadPoolExecutor(max_workers=30) as executor:
        results = list(executor.map(fetch_content, urls))

    for res in results:
        if res:
            stats.append([res["url"], res["count"]])
            all_nodes.extend(res["nodes"])

    # å»é‡å¹¶ä¿å­˜
    unique_nodes = sorted(list(set(all_nodes)))
    
    os.makedirs("results", exist_ok=True)
    with open(OUTPUT_NODES, "w", encoding="utf-8") as f:
        f.write("\n".join(unique_nodes))

    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["è®¢é˜…é“¾æ¥", "æœ‰æ•ˆèŠ‚ç‚¹æ•°"])
        writer.writerows(stats)

    print(f"âœ… æ¸…æ´—å®Œæˆï¼å‰©ä½™çº¯å‡€èŠ‚ç‚¹: {len(unique_nodes)} ä¸ª")

if __name__ == "__main__":
    main()
