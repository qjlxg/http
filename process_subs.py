import requests
import base64
import re
import csv
import os
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

# æ–‡ä»¶è·¯å¾„é…ç½®
INPUT_FILE = "results/subscriptions.txt"
OUTPUT_NODES = "results/nodes.txt"
OUTPUT_CSV = "results/statistics.csv"

def fetch_content(url):
    url = url.strip()
    if not url or url.startswith("#"):
        return None
    
    headers = {
        "User-Agent": "Clash/1.0; v2rayN/6.23" # æ¨¡æ‹Ÿå®¢æˆ·ç«¯
    }
    
    try:
        # å¢åŠ è¶…æ—¶æ§åˆ¶ï¼Œé˜²æ­¢æŸä¸ªæ­»é“¾æ¥å¡ä½è„šæœ¬
        response = requests.get(url, headers=headers, timeout=12)
        if response.status_code == 200:
            raw_data = response.text.strip()
            
            # 1. å°è¯• Base64 è§£ç  (å¤§å¤šæ•°æœºåœºçš„æ ¼å¼)
            try:
                # è¡¥é½ Base64 å¡«å……ç¬¦
                missing_padding = len(raw_data) % 4
                if missing_padding:
                    raw_data += '=' * (4 - missing_padding)
                decoded_data = base64.b64decode(raw_data).decode('utf-8')
                nodes = [n for n in decoded_data.splitlines() if "://" in n]
                return {"url": url, "count": len(nodes), "status": "Success", "nodes": nodes}
            except:
                # 2. å¦‚æœè§£ç å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½œä¸ºæ˜æ–‡å¤„ç† (éƒ¨åˆ† YAML æˆ–å•è¡Œé“¾æ¥)
                nodes = [n for n in raw_data.splitlines() if "://" in n]
                return {"url": url, "count": len(nodes), "status": "Plaintext", "nodes": nodes}
    except Exception as e:
        return {"url": url, "count": 0, "status": "Connect Error", "nodes": []}
    return {"url": url, "count": 0, "status": f"HTTP {response.status_code}", "nodes": []}

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {INPUT_FILE}")
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.startswith("http")]

    print(f"ğŸš€ æ­£åœ¨å¤„ç† {len(urls)} ä¸ªè®¢é˜…é“¾æ¥...")
    
    all_extracted_nodes = []
    stats_data = []

    # ä½¿ç”¨å¹¶å‘åŠ é€Ÿå¤„ç†
    with ThreadPoolExecutor(max_workers=30) as executor:
        results = list(executor.map(fetch_content, urls))

    for res in results:
        if res:
            stats_data.append([res["url"], res["count"], res["status"]])
            all_extracted_nodes.extend(res["nodes"])

    # ç»“æœä¿å­˜
    os.makedirs("results", exist_ok=True)

    # 1. ä¿å­˜ nodes.txt (å»é‡å¤„ç†)
    unique_nodes = list(set(all_extracted_nodes))
    with open(OUTPUT_NODES, "w", encoding="utf-8") as f:
        f.write("\n".join(unique_nodes))

    # 2. ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š CSV
    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["è®¢é˜…é“¾æ¥", "è·å–èŠ‚ç‚¹æ•°", "çŠ¶æ€"])
        writer.writerows(stats_data)

    print(f"âœ… å¤„ç†å®Œæˆ!")
    print(f"ğŸ“ èŠ‚ç‚¹æ–‡ä»¶: {OUTPUT_NODES} (æ€»è®¡ {len(unique_nodes)} ä¸ªå”¯ä¸€èŠ‚ç‚¹)")
    print(f"ğŸ“Š ç»Ÿè®¡æŠ¥è¡¨: {OUTPUT_CSV}")

if __name__ == "__main__":
    main()
