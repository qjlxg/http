import requests
import base64
import re
import socket
import csv
import os
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import json

# é…ç½®
INPUT_FILE = "results/subscriptions.txt"
OUTPUT_NODES = "results/nodes.txt"
OUTPUT_CSV = "results/statistics.csv"

# 1. åè®®ç™½åå•
VALID_PROTOCOLS = ('vmess://', 'vless://', 'ss://', 'ssr://', 'trojan://', 'tuic://', 'hysteria2://', 'hysteria://')
# 2. èŠ‚ç‚¹åé»‘åå•ï¼ˆé‡åˆ°è¿™äº›å…³é”®è¯ç›´æ¥æ‰”æ‰ï¼‰
BAD_KEYWORDS = ['è¿‡æœŸ', 'æµé‡', 'è€—å°½', 'åˆ°æœŸ', '0GB', 'å‰©ä½™', 'å®˜ç½‘', 'æ¸ é“', 'ç»´æŠ¤', 'é‡ç½®']

def check_tcp_alive(node_url):
    """
    æš´åŠ› TCP æ¢æµ‹ï¼šç›´æ¥æ‹¨å·æœåŠ¡å™¨ç«¯å£ã€‚
    """
    host, port = None, None
    try:
        if node_url.startswith(('ss://', 'trojan://', 'vless://')):
            # å¤„ç†æ ¼å¼: protocol://user:pass@host:port#name
            part = node_url.split('@')[1].split('#')[0].split('?')[0]
            host, port = part.split(':')[0], int(part.split(':')[1])
        elif node_url.startswith('vmess://'):
            # Vmess æ˜¯ base64 ç¼–ç çš„ json
            b64_data = node_url.replace('vmess://', '')
            # è¡¥é½å¡«å……
            b64_data += '=' * (-len(b64_data) % 4)
            data = json.loads(base64.b64decode(b64_data).decode('utf-8'))
            host, port = data['add'], int(data['port'])
        
        if host and port:
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            s.settimeout(2) # ç‹ ä¸€ç‚¹ï¼Œ2ç§’ä¸é€šç›´æ¥åˆ¤å®šæ­»åˆ‘
            s.connect((host, port))
            s.close()
            return True
    except:
        pass
    return False

def clean_node(line):
    """æ¸…æ´—ï¼šå»å¹¿å‘Šã€å»é»‘åå•ã€å»æ— æ•ˆè¡Œ"""
    line = line.strip()
    if not line.startswith(VALID_PROTOCOLS): return None
    
    # æ£€æŸ¥é»‘åå•å…³é”®è¯
    for word in BAD_KEYWORDS:
        if word in line: return None
    
    return line

def process_single_sub(url):
    headers = {"User-Agent": "Clash/1.0; v2rayN/6.23"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        if res.status_code != 200: return None
        
        raw = res.text.strip()
        try:
            # è‡ªåŠ¨å°è¯•è¡¥å…¨ base64 å¡«å……
            missing_padding = len(raw) % 4
            if missing_padding: raw += '=' * (4 - missing_padding)
            content = base64.b64decode(raw).decode('utf-8')
        except:
            content = raw
            
        all_lines = content.splitlines()
        alive_nodes = []
        
        for line in all_lines:
            node = clean_node(line)
            if node:
                # åªæœ‰é€šè¿‡ TCP æ¢æµ‹çš„æ‰ç•™ä¸‹
                if check_tcp_alive(node):
                    alive_nodes.append(node)
        
        return {"url": url, "total": len(all_lines), "alive": len(alive_nodes), "nodes": alive_nodes}
    except:
        return {"url": url, "total": 0, "alive": 0, "nodes": []}

def main():
    if not os.path.exists(INPUT_FILE): return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.startswith("http")]

    print(f"ğŸš€ å¯åŠ¨é“è¡€æ”¶å‰²ï¼æ­£åœ¨æ‰«æ {len(urls)} ä¸ªè®¢é˜…æº...")
    
    final_nodes = []
    stat_report = []

    # çº¿ç¨‹æ•°å¯ä»¥å¼€å¤§ç‚¹ï¼ŒTCP æ¢æµ‹å¾ˆå¿«
    with ThreadPoolExecutor(max_workers=50) as executor:
        results = list(executor.map(process_single_sub, urls))

    for r in results:
        if r:
            # åªæœ‰æ´»èŠ‚ç‚¹æ•° > 0 çš„æºæ‰è®°å½•åœ¨ CSV
            stat_report.append([r["url"], r["total"], r["alive"]])
            final_nodes.extend(r["nodes"])

    # å»é‡å¹¶ä¿å­˜
    unique_nodes = list(set(final_nodes))
    os.makedirs("results", exist_ok=True)
    
    with open(OUTPUT_NODES, "w", encoding="utf-8") as f:
        f.write("\n".join(unique_nodes))

    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["è®¢é˜…æºé“¾æ¥", "åŸå§‹è¡Œæ•°", "å­˜æ´»èŠ‚ç‚¹æ•°"])
        # æŒ‰å­˜æ´»æ•°é™åºæ’åˆ—ï¼Œè®©ä½ ä¸€çœ¼çœ‹åˆ°è°æ˜¯çœŸâ€œç²¾å“â€
        stat_report.sort(key=lambda x: x[2], reverse=True)
        writer.writerows(stat_report)

    print(f"âœ… æˆ˜æœæ±‡æŠ¥ï¼šæå– {len(unique_nodes)} ä¸ªçœŸÂ·å­˜æ´»èŠ‚ç‚¹ã€‚å·²å‰”é™¤æ‰€æœ‰åƒåœ¾æ•°æ®ã€‚")

if __name__ == "__main__":
    main()
